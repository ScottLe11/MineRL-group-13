# MineRL Tree-Chopping Deep RL Agent

**Project Status**: âœ… Core Implementation Complete - Ready for Testing
**Timeline**: 1.5 weeks
**Algorithms**: DQN (Double Dueling) and PPO

---

## ğŸ¯ Project Goal

Build a Deep Reinforcement Learning agent that learns to **efficiently chop trees in Minecraft** within episodes.

**Success Criteria**: 
- Consistently collects wood (>80% success rate)
- Stretch goal: Learns to craft and use wooden axe (2Ã— faster mining)

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10
- Java JDK 8 (required for MineRL)

### Installation
```bash
# Create environment
conda create -n minerl-env python=3.10
conda activate minerl-env

# Install dependencies
pip install -r requirements.txt

# Install MineRL v1.0.2 from GitHub
pip install git+https://github.com/minerllabs/minerl@v1.0.2
```

### Run Training
```bash
# DQN training (default)
python scripts/train.py

# With custom config
python scripts/train.py --config config/config.yaml
```

### Run Tests
```bash
pytest tests/ -v
```

### Evaluate a Checkpoint
```bash
python scripts/evaluate.py --checkpoint checkpoints/final_model.pt --episodes 10
```

---

## ğŸ“ Project Structure

```
MineRL-group-13/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml              # All hyperparameters (DQN + PPO)
â”‚
â”œâ”€â”€ wrappers/                    # Environment wrappers
â”‚   â”œâ”€â”€ vision.py                # Frame stacking (84x84 grayscale)
â”‚   â”œâ”€â”€ hold_attack.py           # Attack duration handling
â”‚   â”œâ”€â”€ reward.py                # Reward scaling
â”‚   â”œâ”€â”€ observation.py           # Time/yaw/pitch scalars
â”‚   â””â”€â”€ actions.py               # 23 discrete actions
â”‚
â”œâ”€â”€ networks/                    # Neural network architectures
â”‚   â”œâ”€â”€ cnn.py                   # SmallCNN (84x84 â†’ 512 features)
â”‚   â”œâ”€â”€ dueling_head.py          # Dueling Q-value head
â”‚   â”œâ”€â”€ dqn_network.py           # Full DQN network
â”‚   â””â”€â”€ policy_network.py        # Actor-Critic for PPO
â”‚
â”œâ”€â”€ agent/                       # RL agents
â”‚   â”œâ”€â”€ replay_buffer.py         # Experience replay (DQN)
â”‚   â”œâ”€â”€ dqn.py                   # Double DQN agent
â”‚   â””â”€â”€ ppo.py                   # PPO agent with GAE
â”‚
â”œâ”€â”€ utils/                       # Utilities
â”‚   â”œâ”€â”€ config.py                # Config loader
â”‚   â””â”€â”€ logger.py                # TensorBoard logging
â”‚
â”œâ”€â”€ scripts/                     # Entry points
â”‚   â”œâ”€â”€ train.py                 # Training script
â”‚   â””â”€â”€ evaluate.py              # Evaluation script
â”‚
â”œâ”€â”€ crafting/                    # Tested crafting macros
â”‚   â”œâ”€â”€ crafting_guide.py        # Craft planks/sticks/table/axe
â”‚   â””â”€â”€ gui_clicker.py           # GUI interaction helper
â”‚
â”œâ”€â”€ tests/                       # Unit tests (47 tests)
â”‚   â”œâ”€â”€ test_networks.py
â”‚   â”œâ”€â”€ test_agent.py
â”‚   â””â”€â”€ test_wrappers.py
â”‚
â””â”€â”€ main.py                      # Original demo/reference
```

---

## ğŸ”§ Configuration

All settings in `config/config.yaml`:

```yaml
algorithm: "dqn"                  # "dqn" or "ppo"

environment:
  frame_shape: [84, 84]           # Grayscale frame size
  max_steps: 8000                 # Max steps per episode

dqn:
  num_actions: 23                 # Total discrete actions
  learning_rate: 0.0001
  gamma: 0.99
  batch_size: 32
  replay_buffer:
    capacity: 100000
    min_size: 10000

rewards:
  wood_value: 1.0                 # Points per log
  step_penalty: -0.001            # -0.001 per frame

device: "auto"                    # cpu, cuda, mps, or auto
```

---

## ğŸ® Action Space (23 Actions)

| Index | Action | Frames | Description |
|-------|--------|--------|-------------|
| 0 | noop | 4 | Do nothing |
| 1-4 | movement | 4 | forward, back, right, left |
| 5 | jump | 4 | Jump |
| 6 | attack | 4 | Attack/mine |
| 7-10 | turn_left | 4 | 30Â°, 45Â°, 60Â°, 90Â° |
| 11-14 | turn_right | 4 | 30Â°, 45Â°, 60Â°, 90Â° |
| 15-16 | look_up | 4 | 12Â°, 20Â° |
| 17-18 | look_down | 4 | 12Â°, 20Â° |
| 19 | craft_planks | ~50 | Logs â†’ Planks |
| 20 | make_table | ~100 | Craft + place table |
| 21 | craft_sticks | ~50 | Planks â†’ Sticks |
| 22 | craft_axe | ~100 | Craft + equip axe |

---

## ğŸ§  Observation Space

| Component | Shape | Description |
|-----------|-------|-------------|
| pov | (4, 84, 84) | Stacked grayscale frames |
| time | (1,) | Normalized time remaining [0, 1] |
| yaw | (1,) | Horizontal rotation [-1, 1] |
| pitch | (1,) | Vertical rotation [-1, 1] |

---

## ğŸ’° Reward Function

```
reward_per_frame = (logs Ã— wood_value) + step_penalty
```

- **wood_value** points per log (default: 1.0)
- **step_penalty** per MineRL frame (default: -0.001, so -0.004 per decision)

**Example**: Mine 1 log over 4 frames = `(1 Ã— 1.0) + (-0.001 Ã— 4) = +0.996`

---

## ğŸ”„ Wrapper Stack

```
MineRL Base Environment
    â†“
StackAndProcessWrapper (84x84 grayscale, 4 frames)
    â†“
HoldAttackWrapper (attack duration)
    â†“
RewardWrapper (add step penalty)
    â†“
ObservationWrapper (time, yaw, pitch)
    â†“
ExtendedActionWrapper (23 discrete actions)
    â†“
Agent
```

---

## ğŸ“Š Algorithms

### DQN (Default)
- Double DQN (reduces overestimation)
- Dueling architecture (value + advantage streams)
- Experience replay (100K buffer)
- Epsilon-greedy exploration (1.0 â†’ 0.1)
- Soft target updates (Ï„ = 0.005)

### PPO (Alternative)
- Clipped surrogate objective (Îµ = 0.2)
- GAE advantage estimation (Î» = 0.95)
- Entropy bonus (0.01)
- Rollout buffer (2048 steps)

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_networks.py -v

# Run with coverage
pytest tests/ --cov=.
```

**Test coverage**:
- 47 tests across networks, agent, and wrappers
- Network dimensions, gradient flow, soft updates
- Replay buffer, epsilon schedule, training steps
- Observation wrapper, action wrapper, reward shaping

---

## ğŸ“ˆ Training Flow

1. **Initialize**: Load config, create environment, create agent
2. **Collect**: Agent selects action â†’ env.step() â†’ store experience
3. **Train**: Sample batch â†’ compute loss â†’ update network
4. **Log**: TensorBoard metrics (loss, Q-values, rewards)
5. **Save**: Periodic checkpoints

---

## ğŸ” Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Frame size | 84Ã—84 | Atari standard, good balance |
| Frame stack | 4 | Motion/temporal information |
| Step penalty | Once per decision | Not 4Ã— per frame |
| Macros | Always execute | Learn through experience |
| Inventory | Not observed | Learn from visual hotbar |

---

## ğŸ“š Architecture Document

See `REVISED_ARCHITECTURE.md` for detailed implementation plan.

---

## ğŸ› ï¸ Technology Stack

- **MineRL 1.0.2** - Minecraft environment
- **PyTorch** - Deep learning
- **OpenCV** - Image processing
- **TensorBoard** - Monitoring
- **pytest** - Testing

---

## ğŸ“ License

[Your license here]

---

**Ready to train! ğŸš€ğŸŒ³**
