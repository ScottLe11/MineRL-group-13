# MineRL Tree-Chopping Deep RL Agent

**Project Status**: âœ… Core Implementation Complete - Ready for Testing
**Timeline**: 1.5 weeks
**Algorithms**: DQN (Double Dueling) and PPO

---

## ğŸ¯ Project Goal

Build a Deep Reinforcement Learning agent that learns to **efficiently chop trees in Minecraft** within episodes.

**Success Criteria**: 
- Consistently collects wood (>80% success rate)
- Stretch goal: Learns to craft and use wooden axe (2Ã— faster mining)

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9
- Anaconda
- Homebrew

### Installation
#### Mac Users Installation Guide
```bash
# Java JDK 8 (required for MineRL)
brew tap AdoptOpenJDK/openjdk
brew install --cask adoptopenjdk8 
export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)

# Create environment
conda create --platform osx-64 -n minerl-env python=3.9 -y
conda activate minerl-env

# Install dependencies
git clone https://github.com/minerllabs/minerl.git
sed -i .bak 's/3\.2\.1/3.3.1/' ./minerl/scripts/mcp_patch.diff
cd minerl
pip install .
sed -i .bak s/'java -Xmx\$maxMem'/'java -Xmx\$maxMem -XstartOnFirstThread'/ ./minerl/MCP-Reborn/launchClient.sh
sed -i .bak /'GLFW.glfwSetWindowIcon(this.handle, buffer);'/d ./minerl/MCP-Reborn/src/main/java/net/minecraft/client/MainWindow.java
sed -i .bak '125,136s/^/\/\//' ./minerl/MCP-Reborn/src/main/java/net/minecraft/client/MainWindow.java
cd minerl/MCP-Reborn && ./gradlew clean build shadowJar 
cd ../../../
cp -rf ./minerl/minerl/MCP-Reborn/* 
TARGET_DIR=$(python -c "import site; print(site.getsitepackages()[0])")/minerl/MCP-Reborn/
cp -rf ./minerl/minerl/MCP-Reborn/* "$TARGET_DIR"
pip install -r requirements.txt

# Set up biome
chmod +x scripts/*.sh
./scripts/setup_minerl_environment.sh
```

#### Window Users Installation Guide
```bash
# Java JDK 8 (required for MineRL)
brew tap AdoptOpenJDK/openjdk
brew install --cask adoptopenjdk8 
export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)

# Create environment
conda create -n minerl-env python=3.9
conda activate minerl-env

# Install dependencies
pip install -r requirements.txt

# Install MineRL v1.0.2 from GitHub
pip install git+https://github.com/minerllabs/minerl@v1.0.2

# Set up biome
chmod +x scripts/*.sh
./scripts/setup_minerl_environment.sh
```

### Run Training 
```bash
# Training (default)
python -m scripts.train 

# With window showing agent gameplay
python -m scripts.train --render

# With custom config
python -m scripts.train --config config/config.yaml
```

### Run Tests
```bash
pytest tests/ -v
```

### Evaluate a Checkpoint
```bash
python -m scripts.evaluate --checkpoint checkpoints/final_model.pt --episodes 10
```

---

## ğŸ“ Project Structure

```
MineRL-group-13/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml              # All hyperparameters (DQN + PPO)
â”‚   â””â”€â”€ recording_config.yaml    # Configures settings for recording human gameplay.
â”‚
â”œâ”€â”€ wrappers/                    # Environment wrappers
â”‚   â”œâ”€â”€ vision.py                # Frame stacking (84x84 grayscale)
â”‚   â”œâ”€â”€ hold_attack.py           # Attack duration handling
â”‚   â”œâ”€â”€ reward.py                # Reward scaling
â”‚   â”œâ”€â”€ observation.py           # Time/yaw/pitch scalars
â”‚   â”œâ”€â”€ actions.py               # 21 discrete actions
â”‚   â”œâ”€â”€ discrete_actions.py      # 26 discrete actions.
â”‚   â”œâ”€â”€ frameskip.py             # Repeats actions over multiple frames
â”‚   â””â”€â”€ recorder.py              # Saves gameplay trajectories to files
â”‚
â”œâ”€â”€ networks/                    # Neural network architectures
â”‚   â”œâ”€â”€ attention.py             # Focuses on relevant screen regions
â”‚   â”œâ”€â”€ scalar_network.py.py     # Processes non-visual numeric data
â”‚   â”œâ”€â”€ cnn.py                   # SmallCNN (84x84 â†’ 512 features)
â”‚   â”œâ”€â”€ dueling_head.py          # Dueling Q-value head
â”‚   â”œâ”€â”€ dqn_network.py           # Full DQN network
â”‚   â””â”€â”€ policy_network.py        # Actor-Critic for PPO
â”‚
â”œâ”€â”€ agent/                       # RL agents
â”‚   â”œâ”€â”€ replay_buffer.py         # Experience replay (DQN)
â”‚   â”œâ”€â”€ dqn.py                   # Double DQN agent
â”‚   â””â”€â”€ ppo.py                   # PPO agent with GAE
â”‚
â”œâ”€â”€ best_model/                  # Contains the best checkpoints
â”‚   â””â”€â”€ best_model_ppo_ep2050.pt # Best Checkpoint for ppo (training)
â”‚
â”œâ”€â”€ utils/                       # Utilities
â”‚   â”œâ”€â”€ config.py                # Config loader
â”‚   â”œâ”€â”€ logger.py                # TensorBoard logging
â”‚   â”œâ”€â”€ agent_factory.py         # Creates and configures RL agents
â”‚   â”œâ”€â”€ env_factory.py           # Builds wrapped MineRL environments
â”‚   â”œâ”€â”€ run_grad_cam.py          # Generates Grad-CAM heatmap images
â”‚   â”œâ”€â”€ training_monitoring.py   # Manages real-time training plots
â”‚   â”œâ”€â”€ video_recorder.py        # Records gameplay at training milestones
â”‚   â””â”€â”€ visualization.py         # Utilities for plots and heatmaps
â”‚
â”œâ”€â”€ scripts/                        # Entry points
â”‚   â”œâ”€â”€ train.py                    # Training script
â”‚   â”œâ”€â”€ remove_unwanted_drops.sh    # Removes clutter item drops
â”‚   â”œâ”€â”€ restore_original_jar.sh     # Restores original MineRL JAR
â”‚   â”œâ”€â”€ setup_minerl_environment.sh # Configures biome and drops
â”‚   â”œâ”€â”€ setup_tall_birch_biome.sh   # Forces tall birch forest spawn
â”‚   â”œâ”€â”€ train.py                    # Training script
â”‚   â””â”€â”€ visualize_attention.py      # Saves attention heatmaps from checkpoint
â”‚ 
â”œâ”€â”€ recording/                    # Manages action queuing logic
â”‚   â””â”€â”€ action_queue.py           # Ensures actions finish before new ones start
â”‚ 
â”œâ”€â”€ trainers/                    # Contains training loops 
â”‚   â”œâ”€â”€ helpers.py                # Shared utilities and imitation learning
â”‚   â”œâ”€â”€ train_dqn.py              # DQN algorithm training loop
â”‚   â””â”€â”€ train_ppo.py              # PPO algorithm training loop
â”‚
â”œâ”€â”€ crafting/                    # Tested crafting macros
â”‚   â”œâ”€â”€ crafting_guide.py        # Craft planks/sticks/table/axe
â”‚   â”œâ”€â”€ crafting_utils.py        # Inventory parsing and GUI helpers
â”‚   â””â”€â”€ gui_clicker.py           # GUI interaction helper
â”‚
â”œâ”€â”€ tests/                       # Unit tests (47 tests)
â”‚   â”œâ”€â”€ test_networks.py
â”‚   â”œâ”€â”€ recorder_gameplay.py
â”‚   â””â”€â”€ test_wrappers.py
â”‚
â”œâ”€â”€ pkl_parser.py                # Converts recordings into training data
â”œâ”€â”€ recorder_gameplay.py         # Records gameplay using standard controls
â”œâ”€â”€ treechop_spec.py             # Configurable MineRL tree-chopping environment
â””â”€â”€ main.py                      # Environment registration and vectorization setup
```

---

## ğŸ”§ Configuration

All settings in `config/config.yaml`:

```yaml
algorithm: "dqn"                  # "dqn" or "ppo"

environment:
  frame_shape: [84, 84]           # Grayscale frame size
  max_steps: 8000                 # Max steps per episode

dqn:
  num_actions: 23                 # Total discrete actions
  learning_rate: 0.0001
  gamma: 0.99
  batch_size: 32
  replay_buffer:
    capacity: 100000
    min_size: 10000

rewards:
  wood_value: 1.0                 # Points per log
  step_penalty: -0.001            # -0.001 per frame

device: "auto"                    # cpu, cuda, mps, or auto
```

---

## ğŸ® Action Space (23 Actions)

| Index | Action | Frames | Description |
|-------|--------|--------|-------------|
| 0 | noop | 4 | Do nothing |
| 1-4 | movement | 4 | forward, back, right, left |
| 5 | jump | 4 | Jump |
| 6 | attack | 4 | Attack/mine |
| 7-10 | turn_left | 4 | 30Â°, 45Â°, 60Â°, 90Â° |
| 11-14 | turn_right | 4 | 30Â°, 45Â°, 60Â°, 90Â° |
| 15-16 | look_up | 4 | 12Â°, 20Â° |
| 17-18 | look_down | 4 | 12Â°, 20Â° |
| 19 | craft_planks | ~50 | Logs â†’ Planks |
| 20 | make_table | ~100 | Craft + place table |
| 21 | craft_sticks | ~50 | Planks â†’ Sticks |
| 22 | craft_axe | ~100 | Craft + equip axe |

---

## ğŸ§  Observation Space

| Component | Shape | Description |
|-----------|-------|-------------|
| pov | (4, 84, 84) | Stacked grayscale frames |
| time | (1,) | Normalized time remaining [0, 1] |
| yaw | (1,) | Horizontal rotation [-1, 1] |
| pitch | (1,) | Vertical rotation [-1, 1] |

---

## ğŸ’° Reward Function

```
reward_per_frame = (logs Ã— wood_value) + step_penalty
```

- **wood_value** points per log (default: 1.0)
- **step_penalty** per MineRL frame (default: -0.001, so -0.004 per decision)

**Example**: Mine 1 log over 4 frames = `(1 Ã— 1.0) + (-0.001 Ã— 4) = +0.996`

---

## ğŸ”„ Wrapper Stack

```
MineRL Base Environment
    â†“
StackAndProcessWrapper (84x84 grayscale, 4 frames)
    â†“
HoldAttackWrapper (attack duration)
    â†“
RewardWrapper (add step penalty)
    â†“
ObservationWrapper (time, yaw, pitch)
    â†“
ExtendedActionWrapper (23 discrete actions)
    â†“
Agent
```

---

## ğŸ“Š Algorithms

### DQN (Default)
- Double DQN (reduces overestimation)
- Dueling architecture (value + advantage streams)
- Experience replay (100K buffer)
- Epsilon-greedy exploration (1.0 â†’ 0.1)
- Soft target updates (Ï„ = 0.005)

### PPO (Alternative)
- Clipped surrogate objective (Îµ = 0.2)
- GAE advantage estimation (Î» = 0.95)
- Entropy bonus (0.01)
- Rollout buffer (2048 steps)

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_networks.py -v

# Run with coverage
pytest tests/ --cov=.
```

**Test coverage**:
- 47 tests across networks, agent, and wrappers
- Network dimensions, gradient flow, soft updates
- Replay buffer, epsilon schedule, training steps
- Observation wrapper, action wrapper, reward shaping

---

## ğŸ“ˆ Training Flow

1. **Initialize**: Load config, create environment, create agent
2. **Collect**: Agent selects action â†’ env.step() â†’ store experience
3. **Train**: Sample batch â†’ compute loss â†’ update network
4. **Log**: TensorBoard metrics (loss, Q-values, rewards)
5. **Save**: Periodic checkpoints

---

## ğŸ” Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Frame size | 84Ã—84 | Atari standard, good balance |
| Frame stack | 4 | Motion/temporal information |
| Step penalty | Once per decision | Not 4Ã— per frame |
| Macros | Always execute | Learn through experience |
| Inventory | Not observed | Learn from visual hotbar |

---

## ğŸ“š Architecture Document

See `REVISED_ARCHITECTURE.md` for detailed implementation plan.

---

## ğŸ› ï¸ Technology Stack

- **MineRL 1.0.2** - Minecraft environment
- **PyTorch** - Deep learning
- **OpenCV** - Image processing
- **TensorBoard** - Monitoring
- **pytest** - Testing

---

## ğŸ“ License

[Your license here]

---

**Ready to train! ğŸš€ğŸŒ³**
