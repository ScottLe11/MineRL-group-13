# MineRL Tree-Chopping Deep RL Agent

**Project Status**: âœ… Planning Complete - Ready for Implementation
**Timeline**: 1.5 weeks (12 days)
**Team**: 3 students, 3 machines (2 M1 Macs 8GB, 1 Windows 32GB GPU)

---

## ğŸ¯ Project Goal

Build a Deep Reinforcement Learning agent that learns to **efficiently chop trees in Minecraft** within 60-second episodes.

**Core Challenge**: Agent must learn to:
1. Navigate to trees
2. Mine wood with fists (slow)
3. *Stretch goal*: Craft and use wooden axe (2Ã— faster mining)

**Success Criteria**: 
- Consistently collects wood in 60-second episodes (>80% success rate)
- Stretch: Learns to craft axe for efficiency

---

## ğŸ“š Documentation

**Start here**: [`GETTING_STARTED.md`](GETTING_STARTED.md) - Quick start guide

### Core Documentation

| Document | Purpose | When to Use |
|----------|---------|-------------|
| **IMPLEMENTATION_DECISIONS.md** | âœ… **Finalized design decisions** | **Before implementing** |
| **PROJECT_ARCHITECTURE.md** | Module specifications & interfaces | During implementation |
| **COMPONENT_INTERACTIONS.md** | Visual diagrams of data flow | Understanding system |
| **CRITICAL_QUESTIONS.md** | âœ… All questions resolved | Reference decisions |
| **MODULE_CHECKLIST.md** | Task tracking | Project management |
| **INTEGRATION_PLAN.md** | Build order & testing | Planning work |
| **PROJECT_STATUS.md** | Current state & blockers | Daily updates |
| **DIRECTORY_STRUCTURE.md** | File layout | Navigation |
| **MODULE_SPEC_TEMPLATE.md** | Detailed spec template | Creating specs |
| **GETTING_STARTED.md** | Quick start | New team members |

---

## ğŸš€ Quick Start

### 1. Read Key Documents (30 min)
```bash
1. Read GETTING_STARTED.md (10 min)
2. Skim IMPLEMENTATION_DECISIONS.md (10 min) - KEY DECISIONS
3. Review COMPONENT_INTERACTIONS.md diagrams (10 min)
```

### 2. Set Up Environment (15 min)
```bash
cd /Users/edwinma/Documents/MineRL-group-13

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Mac/Linux

# Install dependencies
pip install minerl==1.0 torch torchvision opencv-python pyyaml tensorboard pytest psutil matplotlib

# Create requirements.txt
pip freeze > requirements.txt
```

### 3. Start Implementation
See [`GETTING_STARTED.md`](GETTING_STARTED.md) for team assignments and parallel development strategy.

---

## ğŸ—ï¸ System Overview

### High-Level Architecture
```
User/Script â†’ Config â†’ Trainer â†’ (Environment, Agent, Logger, Evaluator)
                                        â†“         â†“
                                   (Wrappers)  (Q-Network, Replay Buffer)
```

### Key Components

**Environment** (`environment/`):
- MineRL base + custom wrappers
- 23 discrete actions (primitives, camera, macros)
- Observations: visual (4Ã—64Ã—64) + scalars (time, yaw, pitch)
- Reward: wood collection (+1.0) + step penalty (-0.001)

**Actions** (`actions/`):
- 7 primitives (noop, movement, attack) - 4 frames each
- 12 camera (turn, look) - 4 frames each
- 4 macros (craft planks/sticks/table/axe) - 8-12 frames

**Networks** (`networks/`):
- 5 CNN architectures (tiny â†’ deep)
- Dueling DQN head (value + advantage)
- Optional spatial attention

**Algorithm** (`algorithms/dqn/`):
- Double DQN (reduce overestimation)
- Replay buffer (100K capacity)
- Epsilon-greedy exploration
- Soft target network updates

**Training** (`training/`):
- Phase 1: Reconnaissance (40 configs, 200 episodes, 20s)
- Phase 2: Validation (3-5 configs, 1000 episodes, 60s)
- Phase 3: Final training (best config, 3000-5000 episodes, Gorila)

---

## ğŸ”‘ Key Design Decisions

> **See [`IMPLEMENTATION_DECISIONS.md`](IMPLEMENTATION_DECISIONS.md) for complete details**

### Critical Choices

1. **Temporal Abstraction**: 1 decision = 4 frames = 200ms
   - Agent decides at 5 Hz, MineRL runs at 20 Hz
   - ONE experience per decision (not per frame)

2. **Observation Space**: Visual + 3 Scalars
   - Visual: (4, 64, 64) grayscale stacked frames
   - Scalars: time_left, yaw, pitch
   - **NO inventory** (learns from visual hotbar)

3. **Reward Function**: Wood Only
   - +1.0 per wood collected
   - -0.001 per step
   - **NO crafting sub-rewards** (learns instrumental value)

4. **Macro Behavior**: Always Execute
   - Macros run full sequence even without materials
   - Agent learns through experience not to craft without items

5. **Action Space**: 23 Actions
   - 0-6: Primitives (movement, attack)
   - 7-18: Camera (turn/look)
   - 19-22: Crafting macros

---

## ğŸ“Š Three-Phase Training

### Phase 1: Reconnaissance (Days 2-3)
- **Goal**: Find 8 configs that get 1+ wood
- **Method**: 40 configs Ã— 200 episodes Ã— 20s
- **Execution**: 3 machines parallel (overnight)
- **Output**: Comparison CSV, select top 3-5

### Phase 2: Validation (Days 4-5)
- **Goal**: Confirm configs work on 60s episodes
- **Method**: 3-5 configs Ã— 1000 episodes Ã— 60s
- **Execution**: Windows machine sequential
- **Output**: Best config identified

### Phase 3: Final Training (Days 6-9)
- **Goal**: Train to convergence
- **Method**: Best config Ã— 3000-5000 episodes Ã— 60s
- **Execution**: Gorila distributed (4 actors, 2 learners)
- **Output**: Final model, videos, analysis

---

## ğŸ¯ Success Metrics

### Phase 1 (Recon)
- âœ… At least 3 configs show learning
- âœ… At least 1 config gets wood in >10% episodes

### Phase 2 (Validation)
- âœ… Best config gets wood in >50% episodes
- âœ… Stable learning curve

### Phase 3 (Final)
- âœ… Final agent gets wood in >80% episodes
- âœ… Average 2+ wood per episode
- ğŸ Stretch: Agent learns to craft & use axe

### Comparison to Baseline
- Random agent: ~0.3% success rate (60s episodes)
- Expected: >80% success rate (267Ã— better!)

---

## ğŸ—“ï¸ Timeline

| Days | Phase | Activities |
|------|-------|-----------|
| **1-2** | Infrastructure | Build modules, tests, configs |
| **2-3** | Recon | Run 40 configs on 3 machines |
| **4** | Analysis | Compare results, select top configs |
| **4-5** | Validation | Validate on 60s episodes |
| **6-9** | Final Training | Train best config with Gorila |
| **10-11** | Analysis | Generate visualizations, report |
| **12** | Buffer | Polish, fix issues |

**Current Status**: End of Day 1 â†’ Day 2 (Infrastructure)

---

## ğŸ› ï¸ Technology Stack

**Core**:
- MineRL 1.0 (Minecraft environment)
- PyTorch (deep learning)
- OpenCV (image processing)

**RL Algorithm**:
- Double DQN with Dueling Architecture
- Experience Replay (100K capacity)
- Epsilon-greedy exploration

**Infrastructure**:
- TensorBoard (monitoring)
- YAML (configuration)
- pytest (testing)

**Optional**:
- Gorila distributed training (Phase 3)
- Human demonstrations (behavior cloning)
- Curriculum learning

---

## ğŸ“ Project Structure

```
MineRL-group-13/
â”œâ”€â”€ ğŸ“„ Documentation (10 files) âœ… COMPLETE
â”œâ”€â”€ ğŸ“ environment/      (Environment & wrappers)
â”œâ”€â”€ ğŸ“ actions/          (23 action definitions)
â”œâ”€â”€ ğŸ“ networks/         (CNN architectures)
â”œâ”€â”€ ğŸ“ algorithms/       (DQN implementation)
â”œâ”€â”€ ğŸ“ training/         (3-phase pipeline)
â”œâ”€â”€ ğŸ“ utils/            (Config, logging, checkpointing)
â”œâ”€â”€ ğŸ“ scripts/          (Entry points)
â”œâ”€â”€ ğŸ“ tests/            (Test suite)
â”œâ”€â”€ ğŸ“ evaluation/       (Evaluation & comparison)
â”œâ”€â”€ ğŸ“ distributed/      (Gorila - optional)
â”œâ”€â”€ ğŸ“ demonstrations/   (Demo parsing - optional)
â”œâ”€â”€ ğŸ“ config/           (YAML configurations)
â””â”€â”€ ğŸ“ crafting/         (Macro implementations) âœ… EXISTS
```

**Status**: 18 files exist, ~127 to create

---

## ğŸ” Key Features

### Temporal Abstraction
- Agent makes decisions every 200ms (not every 50ms frame)
- Reduces action space complexity
- Enables macro actions (crafting sequences)

### Dueling DQN Architecture
- Separates value (how good is state) from advantage (how good is action)
- More stable learning
- Better generalization

### Double DQN
- Reduces Q-value overestimation
- Selects action with online network
- Evaluates with target network

### Optional: Gorila Distributed Training
- 4 parallel actors generate experiences
- 2 learners compute gradients
- 1 parameter server coordinates updates
- 3-4Ã— speedup over single agent

---

## ğŸ“ Learning Challenges

### Why This is Hard
1. **Sparse Rewards**: Only get reward when collecting wood
2. **Long Horizons**: 300 decisions per episode (60s)
3. **Visual Input**: Must learn from pixels
4. **Instrumental Learning**: Must discover crafting helps mining
5. **Exploration**: Vast action/state space

### How We Address It
1. **Step Penalty**: Encourages faster wood collection
2. **Frame Stacking**: Provides motion/temporal info
3. **CNN Features**: Learns visual representations
4. **Large Replay Buffer**: Learns from past experiences
5. **Epsilon Decay**: Gradual explorationâ†’exploitation

---

## ğŸ¤ Contributing

### For Team Members

**Implementing a module?**
1. Read relevant section in `PROJECT_ARCHITECTURE.md`
2. Check `IMPLEMENTATION_DECISIONS.md` for design choices
3. Use `MODULE_SPEC_TEMPLATE.md` for detailed design
4. Write tests in `tests/`
5. Update `MODULE_CHECKLIST.md` when complete

**Stuck?**
1. Check documentation (especially COMPONENT_INTERACTIONS.md)
2. Run relevant tests
3. Ask in team channel with details (error messages, what you tried)

---

## ğŸ“– References

### Papers
- DQN: [Mnih et al. 2015](https://www.nature.com/articles/nature14236)
- Double DQN: [van Hasselt et al. 2015](https://arxiv.org/abs/1509.06461)
- Dueling DQN: [Wang et al. 2015](https://arxiv.org/abs/1511.06581)
- Prioritized Replay: [Schaul et al. 2015](https://arxiv.org/abs/1511.05952)
- Gorila: [Nair et al. 2015](https://arxiv.org/abs/1507.04296)

### Documentation
- MineRL: https://minerl.readthedocs.io/en/v1.0/
- PyTorch: https://pytorch.org/docs/
- OpenAI Gym: https://www.gymlibrary.dev/

---

## ğŸ“ License

[Your license here]

---

## ğŸ‰ Status

**âœ… Planning Complete** (2025-11-24)
- All design decisions finalized
- Architecture documented
- Ready for implementation

**Next**: Begin parallel module development (Day 2)

---

**Questions?** See `GETTING_STARTED.md` or check `PROJECT_STATUS.md` for current state.

**Let's build this! ğŸš€ğŸŒ³**

