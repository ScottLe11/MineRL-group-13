# MineRL Tree-Chopping RL Agent Configuration
# Supports both DQN and PPO algorithms

# =============================================================================
# ENVIRONMENT
# =============================================================================
environment:
  name: "MineRLcustom_treechop-v0"
  
  # Episode length
  # - Each agent step = 4 frames = 200ms = 0.2 seconds
  # - So 1 second = 5 agent steps
  # - Recon: 20 seconds, Full: 60 seconds
  episode_seconds: 20                # Episode duration (20s recon, 60s full)
  # max_steps is computed: episode_seconds * 5
  
  # Visual processing
  frame_shape: [84, 84]              # Grayscale frame size (matches Atari standard)
  frame_stack: 4                     # Number of stacked frames
  
  # Curriculum (starting conditions) - see config/CURRICULUM_LEARNING.md
  curriculum:
    spawn_type: "random"             # "random" or "near_tree" (near_tree needs custom handler)
    with_logs: 8                     # Number of starting logs (0-10)
    with_axe: true                  # Start with wooden axe equipped

# =============================================================================
# NETWORK
# =============================================================================
network:
  input_channels: 4                  # Frame stack size
  cnn_features: 512                  # CNN output features
  
  # CNN Architecture selection
  # Options: 'tiny' (~150K), 'small' (~400K), 'medium' (~600K), 'wide' (~1M), 'deep' (~500K)
  architecture: "small"              # Default: small (Atari DQN style)
  
# =============================================================================
# DQN AGENT
# =============================================================================
dqn:
  num_actions: 23                    # Total discrete actions
  
  # Learning
  learning_rate: 0.0001              # Adam learning rate
  gamma: 0.99                        # Discount factor
  batch_size: 32                     # Training batch size
  gradient_clip: 10.0                # Max gradient norm
  
  # Replay Buffer
  replay_buffer:
    capacity: 100000                 # Buffer size (experiences)
    min_size: 10000                  # Min experiences before training
  
  # Exploration (epsilon-greedy)
  # NOTE: decay_steps is in AGENT STEPS, not episodes
  # - Recon (200 eps × 300 steps = 60k): use 50000 for full decay
  # - Full (1000 eps × 300 steps = 300k): use 250000 for gradual decay
  exploration:
    epsilon_start: 1.0               # Starting epsilon
    epsilon_end: 0.05                 # Final epsilon
    epsilon_decay_steps: 50000       # Steps for linear decay
  
  # Target Network
  target_update:
    method: "soft"                   # "soft" or "hard"
    tau: 0.005                       # Soft update coefficient
    hard_update_freq: 1000           # Steps between hard updates (if hard)
  
  # Prioritized Experience Replay (optional)
  prioritized_replay:
    enabled: false                   # Use PER instead of uniform sampling
    alpha: 0.6                       # Priority exponent (0=uniform, 1=full prioritization)
    beta_start: 0.4                  # Initial importance sampling exponent
    beta_end: 1.0                    # Final importance sampling exponent (anneals with epsilon)

# =============================================================================
# TRAINING
# =============================================================================
training:
  # Training duration (in EPISODES, not steps)
  # - Recon phase: 200 episodes (quick hyperparameter search)
  # - Full training: 1000-3000 episodes
  num_episodes: 200                  # Total episodes to train
  train_freq: 4                      # Train every N agent steps
  
  # Logging & Saving (in episodes)
  log_freq: 10                       # Log metrics every N episodes
  save_freq: 50                      # Save checkpoint every N episodes
  eval_freq: 50                      # Evaluate every N episodes
  eval_episodes: 5                   # Episodes per evaluation
  
  # Paths
  checkpoint_dir: "checkpoints"
  log_dir: "runs"

# =============================================================================
# PPO (Alternative to DQN)
# =============================================================================
ppo:
  learning_rate: 0.0003              # Typically higher than DQN
  gamma: 0.99                        # Discount factor
  gae_lambda: 0.95                   # GAE lambda
  clip_epsilon: 0.2                  # PPO clipping parameter
  entropy_coef: 0.01                 # Entropy bonus
  value_coef: 0.5                    # Value loss coefficient
  max_grad_norm: 0.5                 # Gradient clipping
  n_steps: 2048                      # Steps per rollout
  n_epochs: 10                       # Epochs per update
  batch_size: 64                     # Minibatch size

# =============================================================================
# ALGORITHM SELECTION
# =============================================================================
algorithm: "dqn"                     # "dqn" or "ppo"

# =============================================================================
# REWARD
# =============================================================================
rewards:
  wood_value: 1.0                    # Points per log mined
  step_penalty: -0.001               # Penalty per frame (encourages speed)

# =============================================================================
# HARDWARE
# =============================================================================
device: "auto"                       # "cpu", "cuda", "mps", or "auto"
seed: null                           # Random seed (null for random)


