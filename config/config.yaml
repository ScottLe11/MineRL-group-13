# MineRL Tree-Chopping RL Agent Configuration
# Supports DQN, PPO, and Behavioral Cloning algorithms
# =============================================================================
# BEHAVIORAL CLONING (BC) & IMITATION LEARNING
# =============================================================================
bc:
  # Path to the data file generated by pkl_parser.py
  data_path: "bc_expert_data.npz"
  # Supervised learning rate for the policy network
  learning_rate: 0.0003
  # Batch size for processing expert data
  batch_size: 64
  # Max gradient norm during optimization
  gradient_clip: 1.0

  # DQfD (Deep Q-Learning from Demonstrations) - Advanced Imitation Learning
  # Loss weights for multi-objective training:
  lambda_supervised: 1.0      # Supervised loss (cross-entropy on expert actions)
  lambda_td: 1.0              # TD loss (Bellman error, requires next_state)
  lambda_margin: 1.0          # Large margin classification loss
  lambda_l2: 0.00001          # L2 regularization weight
  margin: 0.8                 # Margin for ensuring expert actions have higher Q-values

# =============================================================================
# ENVIRONMENT
# =============================================================================
environment:
  name: "MineRLcustom_treechop-v0"

  # Episode length
  # - Each agent step = 4 frames = 200ms = 0.2 seconds
  # - So 1 second = 5 agent steps
  # - Recon: 20 seconds, Full: 60 seconds
  episode_seconds: 60                # Episode duration (20s recon, 60s full)
  # max_steps is computed: episode_seconds * 5

  # Visual processing
  frame_shape: [84, 84]              # Grayscale frame size (matches Atari standard)
  frame_stack: 4                     # Number of stacked frames

  # JAR Modifications (Optional, see scripts/setup_minerl_environment.sh)
  # To optimize the environment for tree-chopping training:
  #   1. Run: bash scripts/setup_minerl_environment.sh
  #   2. This sets tall birch forest biome (dense trees everywhere)
  #   3. Removes unwanted drops (flowers, seeds, saplings, etc.)
  # With JAR mods, spawn_type becomes effectively "near_tree"
  # To restore: bash scripts/restore_original_jar.sh

  # Curriculum (starting conditions) - see config/CURRICULUM_LEARNING.md
  curriculum:
    spawn_type: "random"             # "random" or "near_tree" (near_tree needs custom handler)
    with_logs: 0                    # Number of starting logs (0-10)
    with_axe: false                  # Start with wooden axe equipped

# =============================================================================
# NETWORK
# =============================================================================
network:
  input_channels: 4                  # Frame stack size

  # CNN Architecture selection
  # Options: 'tiny' (~150K), 'small' (~400K), 'medium' (~600K), 'wide' (~1M), 'deep' (~500K)
  # CNN output dimension is architecture-dependent (256 for tiny, 512 for others)
  architecture: "medium"              # Default: small (Atari DQN style, proven baseline)

  # Attention mechanism (optional)
  # Options: 'none', 'spatial', 'cbam', 'treechop_bias'
  # - none: No attention (default, recommended for initial training)
  # - spatial: Pure spatial attention (learns where to look)
  # - cbam: Combined channel + spatial (full CBAM module, most comprehensive)
  # - treechop_bias: Spatial attention biased toward screen center and hotbar
  attention: "cbam"
  
  # Scalar network (optional)
  use_scalar_network: true            # Enable extra FC layer for scalar processing
  scalar_hidden_dim: 64               # Hidden layer size in scalar network
  scalar_output_dim: 64               # Output dimension (fused with CNN features)
# =============================================================================
# ACTION SPACE
# =============================================================================
action_space:
  # Configure which actions to include from the action pool
  # The action pool consists of 25 actions (indices 0-24):
  #   0-6:   Movement primitives (noop, forward, back, right, left, jump, attack)
  #   7-18:  Camera primitives (turn left/right, look up/down at various angles)
  #   19-21: Basic crafting macros (planks, sticks, craft_table_and_axe)
  #   22:    craft_entire_axe (full pipeline: planks -> table -> sticks -> axe)
  #   23:    attack_5 (attack for 5 steps = 20 frames)
  #   24:    attack_10 (attack for 10 steps = 40 frames)

  # Method for action selection:
  # - "base": Use base 22 actions (indices 0-21)
  # - "assisted": Curated set for assisted learning (movement + key camera angles + craft_entire_axe + extended attacks)
  # - "custom": Use custom list specified in enabled_actions
  preset: "custom"                   # Default: assisted learning preset

  # Custom action list (only used if preset is "custom")
  # Example: [0, 1, 2, 6, 7, 8, 23] would include only noop, forward, back,
  #          attack, turn_left_30, turn_left_45, and craft_entire_axe
  enabled_actions: [0, 1, 2, 3, 4, 5, 6, 7, 10, 11, 14, 16, 18, 19, 20]

# =============================================================================
# DQN AGENT
# =============================================================================
dqn:
  num_actions: null                  # Auto-computed from action_space config
  
  # Learning
  learning_rate: 0.0001              # Adam learning rate
  gamma: 0.99                        # Discount factor
  batch_size: 64                     # Training batch size
  gradient_clip: 5                # Max gradient norm
  
  # Replay Buffer
  replay_buffer:
    capacity: 40000                 # Buffer size (experiences)
    min_size: 4000                  # Min experiences before training
  
  # Exploration (epsilon-greedy)
  # NOTE: decay_steps is in AGENT STEPS, not episodes
  # - Recon (200 eps × 300 steps = 60k): use 50000 for full decay
  # - Full (1000 eps × 300 steps = 300k): use 250000 for gradual decay
  exploration:
    epsilon_start: 0.95               # Starting epsilon
    epsilon_end: 0.05                 # Final epsilon
    epsilon_decay_steps: 40000       # Steps for linear decay
  
  # Target Network
  target_update:
    method: "hard"                   # "soft" or "hard"
    tau: 0.005                       # Soft update coefficient
    hard_update_freq: 400           # Steps between hard updates (if hard)
  
  # Prioritized Experience Replay (optional)
  prioritized_replay:
    enabled: true                   # Use PER instead of uniform sampling
    alpha: 0.6                       # Priority exponent (0=uniform, 1=full prioritization)
    beta_start: 0.4                  # Initial importance sampling exponent
    beta_end: 1.0                    # Final importance sampling exponent (anneals with epsilon)

# =============================================================================
# TRAINING
# =============================================================================
training:
  # Training duration (in EPISODES, not steps)
  # - Recon phase: 200 episodes (quick hyperparameter search)
  # - Full training: 1000-3000 episodes
  num_episodes: 2100                  # Total episodes to train
  train_freq: 5                      # Train every N agent steps
  
  # Logging & Saving (in episodes)
  log_freq: 5                        # Log metrics every N episodes
  save_freq: 50                      # Save checkpoint every N episodes
  eval_freq: 50                      # Evaluate every N episodes
  eval_episodes: 5                   # Episodes per evaluation

  # Environment recreation (prevent MineRL memory leaks)
  env_recreation_interval: 25        # Recreate env every N episodes (lower if timeouts persist)

  # Paths
  checkpoint_dir: "checkpoints"
  log_dir: "runs"
  
  # Video recording at milestones (0%, 20%, 40%, 60%, 80%, 100%)
  video_recording:
    enabled: false                   # Record episodes at milestones
    save_dir: "videos"               # Directory for saved videos
    fps: 10                          # Video framerate

# =============================================================================
# PPO (Alternative to DQN)
# =============================================================================
ppo:
  learning_rate: 0.0001              # Typically higher than DQN
  gamma: 0.99                        # Discount factor
  gae_lambda: 0.95                   # GAE lambda
  clip_epsilon: 0.2                  # PPO clipping parameter
  entropy_coef: 0.0                 # Entropy bonus
  value_coef: 0.5                    # Value loss coefficient
  max_grad_norm: 0.5                 # Gradient clipping
  n_steps: 1024                      # Steps per rollout
  n_epochs: 6                       # Epochs per update
  batch_size: 64                     # Minibatch size

# =============================================================================
# ALGORITHM SELECTION
# =============================================================================
# Options:
#   - "dqn": Deep Q-Network (value-based RL)
#   - "ppo": Proximal Policy Optimization (policy-based RL)
#   - "bc_dqn": Behavioral Cloning with DQN (simple: cross-entropy loss only)
#               → Same agent as 'dqn', different training
#   - "bc_ppo": Behavioral Cloning with PPO (simple: cross-entropy loss only)
#               → Same agent as 'ppo', different training
#   - "dqfd": Deep Q-Learning from Demonstrations (advanced: multi-objective loss)
#             → Same agent as 'dqn', different training (supervised + TD + margin + L2)
algorithm: "ppo"

# =============================================================================
# REWARD (Wood Inventory-Based)
# =============================================================================
# New reward system based on wood inventory changes:
# - Reward = (wood_delta × wood_value) + step_penalty
# - Mining wood: +wood_value per log gained
# - Crafting/placing wood: -wood_value per log used
# - Total episode reward = final_wood_count × wood_value + step_penalties
#
# Example with wood_value=1.0:
#   - Mine 10 logs: +10.0
#   - Use 3 logs for crafting: -3.0
#   - Final inventory: 7 logs
#   - Total wood reward: +7.0 (same as final inventory!)
#
rewards:
  wood_value: 5.0                    # Points per log change (mined or used)
  step_penalty: -0.001               # Penalty per frame (encourages speed)

# =============================================================================
# HARDWARE
# =============================================================================
device: "auto"                       # "cpu", "cuda", "mps", or "auto"
seed: null                           # Random seed (null for random)


