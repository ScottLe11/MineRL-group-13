# MineRL Tree-Chopping RL Agent Configuration
# Supports both DQN and PPO algorithms

# =============================================================================
# ENVIRONMENT
# =============================================================================
environment:
  name: "MineRLcustom_treechop-v0"
  max_steps: 8000                    # Max steps per episode
  frame_shape: [64, 64]              # Grayscale frame size
  frame_stack: 4                     # Number of stacked frames

# =============================================================================
# NETWORK
# =============================================================================
network:
  input_channels: 4                  # Frame stack size
  cnn_features: 512                  # CNN output features
  
# =============================================================================
# DQN AGENT
# =============================================================================
dqn:
  num_actions: 23                    # Total discrete actions
  
  # Learning
  learning_rate: 0.0001              # Adam learning rate
  gamma: 0.99                        # Discount factor
  batch_size: 32                     # Training batch size
  gradient_clip: 10.0                # Max gradient norm
  
  # Replay Buffer
  replay_buffer:
    capacity: 100000                 # Buffer size (experiences)
    min_size: 10000                  # Min experiences before training
  
  # Exploration (epsilon-greedy)
  exploration:
    epsilon_start: 1.0               # Starting epsilon
    epsilon_end: 0.1                 # Final epsilon
    epsilon_decay_steps: 100000      # Steps for linear decay
  
  # Target Network
  target_update:
    method: "soft"                   # "soft" or "hard"
    tau: 0.005                       # Soft update coefficient
    hard_update_freq: 1000           # Steps between hard updates (if hard)

# =============================================================================
# TRAINING
# =============================================================================
training:
  total_steps: 1000000               # Total environment steps
  train_freq: 4                      # Train every N steps
  
  # Logging
  log_freq: 1000                     # Log metrics every N steps
  save_freq: 50000                   # Save checkpoint every N steps
  eval_freq: 25000                   # Evaluate every N steps
  eval_episodes: 5                   # Episodes per evaluation
  
  # Paths
  checkpoint_dir: "checkpoints"
  log_dir: "runs"

# =============================================================================
# PPO (Alternative to DQN)
# =============================================================================
ppo:
  learning_rate: 0.0003              # Typically higher than DQN
  gamma: 0.99                        # Discount factor
  gae_lambda: 0.95                   # GAE lambda
  clip_epsilon: 0.2                  # PPO clipping parameter
  entropy_coef: 0.01                 # Entropy bonus
  value_coef: 0.5                    # Value loss coefficient
  max_grad_norm: 0.5                 # Gradient clipping
  n_steps: 2048                      # Steps per rollout
  n_epochs: 10                       # Epochs per update
  batch_size: 64                     # Minibatch size

# =============================================================================
# ALGORITHM SELECTION
# =============================================================================
algorithm: "dqn"                     # "dqn" or "ppo"

# =============================================================================
# REWARD SHAPING
# =============================================================================
rewards:
  wood_collected: 1.0                # Reward per wood collected
  step_penalty: -0.001               # Penalty per step
  # Note: No crafting rewards per design decision

# =============================================================================
# HARDWARE
# =============================================================================
device: "auto"                       # "cpu", "cuda", "mps", or "auto"
seed: 42                             # Random seed (null for random)


