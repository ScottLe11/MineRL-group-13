# MineRL Tree-Chopping Deep RL Agent


## ğŸ¯ Project Goal

Build a Deep Reinforcement Learning agent that learns to **efficiently chop trees in Minecraft** within episodes.

**Success Criteria**: 
- Consistently collects wood (>80% success rate)
- Stretch goal: Learns to craft and use wooden axe (2Ã— faster mining)

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9
- Anaconda
- Homebrew

### Installation
#### Mac Users Installation Guide
```bash
# Java JDK 8 (required for MineRL)
brew tap AdoptOpenJDK/openjdk
brew install --cask adoptopenjdk8 
export JAVA_HOME=$(/usr/libexec/java_home -v 1.8)

# Create environment
conda create --platform osx-64 -n minerl-env python=3.9 -y
conda activate minerl-env

# Install dependencies
git clone https://github.com/minerllabs/minerl.git
sed -i .bak 's/3\.2\.1/3.3.1/' ./minerl/scripts/mcp_patch.diff
cd minerl
pip install .
sed -i .bak s/'java -Xmx\$maxMem'/'java -Xmx\$maxMem -XstartOnFirstThread'/ ./minerl/MCP-Reborn/launchClient.sh
sed -i .bak /'GLFW.glfwSetWindowIcon(this.handle, buffer);'/d ./minerl/MCP-Reborn/src/main/java/net/minecraft/client/MainWindow.java
sed -i .bak '125,136s/^/\/\//' ./minerl/MCP-Reborn/src/main/java/net/minecraft/client/MainWindow.java
cd minerl/MCP-Reborn && ./gradlew clean build shadowJar 
cd ../../../
cp -rf ./minerl/minerl/MCP-Reborn/* 
TARGET_DIR=$(python -c "import site; print(site.getsitepackages()[0])")/minerl/MCP-Reborn/
cp -rf ./minerl/minerl/MCP-Reborn/* "$TARGET_DIR"
pip install -r requirements.txt

# Set up biome
./scripts/setup_minerl_environment.sh
```

#### Window Users Installation Guide
```bash
# Java JDK 8 (required for MineRL)
Go to Oracle and download Java 1.8.0

# Create environment
conda create -n minerl-env python=3.9
conda activate minerl-env

# Install dependencies
pip install -r requirements.txt

# Install MineRL v1.0.2 from GitHub
pip install git+https://github.com/minerllabs/minerl@v1.0.2

# Set up biome
./scripts/setup_minerl_environment.sh
```

### Run Training 
```bash
# Training (default)
python -m scripts.train 

# With window showing agent gameplay
python -m scripts.train --render

# Resume from a checkpoint
python -m scripts.train --resume best_model/checkpoint_ppo_ep3000.pt --render
```

### Evaluate a Checkpoint
```bash
python -m scripts.evaluate --checkpoint best_model/checkpoint_ppo_ep3000.pt --algorithm ppo
```
---

## ğŸ“ Project Structure

```
MineRL-group-13/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml              # All hyperparameters (DQN + PPO)
â”‚   â””â”€â”€ recording_config.yaml    # Configures settings for recording human gameplay.
â”‚
â”œâ”€â”€ wrappers/                    # Environment wrappers
â”‚   â”œâ”€â”€ vision.py                # Frame stacking (84x84 grayscale)
â”‚   â”œâ”€â”€ hold_attack.py           # Attack duration handling
â”‚   â”œâ”€â”€ reward.py                # Reward scaling
â”‚   â”œâ”€â”€ observation.py           # Time/yaw/pitch scalars
â”‚   â”œâ”€â”€ actions.py               #
â”‚   â”œâ”€â”€ discrete_actions.py      # 
â”‚   â”œâ”€â”€ frameskip.py             # Repeats actions over multiple frames
â”‚   â””â”€â”€ recorder.py              # Saves gameplay trajectories to files
â”‚
â”œâ”€â”€ networks/                    # Neural network architectures
â”‚   â”œâ”€â”€ attention.py             # Focuses on relevant screen regions
â”‚   â”œâ”€â”€ scalar_network.py.py     # Processes non-visual numeric data
â”‚   â”œâ”€â”€ cnn.py                   # All CNN architectures 
â”‚   â”œâ”€â”€ dueling_head.py          # Dueling Q-value head
â”‚   â”œâ”€â”€ dqn_network.py           # Full DQN network
â”‚   â””â”€â”€ policy_network.py        # Actor-Critic for PPO
â”‚
â”œâ”€â”€ agent/                       # RL agents
â”‚   â”œâ”€â”€ replay_buffer.py         # Experience replay (DQN)
â”‚   â”œâ”€â”€ dqn.py                   # Double DQN agent
â”‚   â””â”€â”€ ppo.py                   # PPO agent with GAE
â”‚
â”œâ”€â”€ best_model/                  # Contains the best checkpoints
â”‚   â”œâ”€â”€ checkpoint_ppo_ep2500.pt # Best Checkpoint for ppo with bias towards crafting
â”‚   â”œâ”€â”€ checkpoint_ppo_ep3000.pt # Best Checkpoint for ppo with complete action and good at chopping trees
â”‚   â””â”€â”€ best_model_ppo_ep2050.pt # Best Checkpoint for ppo with 6 action space(training)
â”‚
â”œâ”€â”€ utils/                       # Utilities
â”‚   â”œâ”€â”€ config.py                # Config loader
â”‚   â”œâ”€â”€ logger.py                # TensorBoard logging
â”‚   â”œâ”€â”€ agent_factory.py         # Creates and configures RL agents
â”‚   â”œâ”€â”€ env_factory.py           # Builds wrapped MineRL environments
â”‚   â”œâ”€â”€ run_grad_cam.py          # Generates Grad-CAM heatmap images
â”‚   â”œâ”€â”€ training_monitoring.py   # Manages real-time training plots
â”‚   â”œâ”€â”€ video_recorder.py        # Records gameplay at training milestones
â”‚   â””â”€â”€ visualization.py         # Utilities for plots and heatmaps
â”‚
â”œâ”€â”€ scripts/                              # Entry points
â”‚   â”œâ”€â”€ train.py                          # Training script
â”‚   â”œâ”€â”€ evaluate.py                       # Evaluates trained RL agents
â”‚   â”œâ”€â”€ remove_unwanted_drops.sh          # Removes clutter item drops
â”‚   â”œâ”€â”€ restore_original_jar.sh           # Restores original MineRL JAR
â”‚   â”œâ”€â”€ setup_minerl_environment.sh       # Configures biome and drops
â”‚   â”œâ”€â”€ setup_tall_birch_biome.sh         # Forces tall birch forest spawn
â”‚   â”œâ”€â”€ prepare_transfer_for_training.py  # Prepares checkpoint for PPO training
â”‚   â”œâ”€â”€ transfer_learning.py:             # Maps weights to new actions
â”‚   â”œâ”€â”€ verify_transfer.py                # Tests model loading and inference
â”‚   â””â”€â”€ visualize_attention.py            # Saves attention heatmaps from checkpoint
â”‚ 
â”œâ”€â”€ recording/                    # Manages action queuing logic
â”‚   â””â”€â”€ action_queue.py           # Ensures actions finish before new ones start
â”‚ 
â”œâ”€â”€ trainers/                    # Contains training loops 
â”‚   â”œâ”€â”€ helpers.py                # Shared utilities and imitation learning
â”‚   â”œâ”€â”€ train_dqn.py              # DQN algorithm training loop
â”‚   â””â”€â”€ train_ppo.py              # PPO algorithm training loop
â”‚
â”œâ”€â”€ crafting/                    # Tested crafting macros
â”‚   â”œâ”€â”€ crafting_guide.py        # Craft planks/sticks/table/axe
â”‚   â”œâ”€â”€ crafting_utils.py        # Inventory parsing and GUI helpers
â”‚   â””â”€â”€ gui_clicker.py           # GUI interaction helper
â”‚
â”œâ”€â”€ pkl_parser.py                  # Converts recordings into training data
â”œâ”€â”€ recorder_gameplay_discrete.py  # Records gameplay using standard controls
â”œâ”€â”€ treechop_spec.py               # Configurable MineRL tree-chopping environment
â””â”€â”€ main.py                        # Environment registration and vectorization setup
```

---

## ğŸ”§ Configuration

All settings in `config/config.yaml`:
---

## ğŸ® Action Space (22 Actions)

| Index | Action | Frames | Description |
|-------|--------|--------|-------------|
| 0 | noop | 4 | Do nothing |
| 1-4 | movement | 4 | forward, back, right, left |
| 5 | jump | 4 | Jump |
| 6 | attack | 4 | Attack/mine |
| 7-10 | turn_left | 4 | 30Â°, 45Â°, 60Â°, 90Â° |
| 11-14 | turn_right | 4 | 30Â°, 45Â°, 60Â°, 90Â° |
| 15-16 | look_up | 4 | 12Â°, 20Â° |
| 17-18 | look_down | 4 | 12Â°, 20Â° |
| 19 | craft_planks | ~50 | Logs â†’ Planks |
| 20 | craft_sticks | ~50 | Planks â†’ Sticks |
| 21 | CRAFT_TABLE_AND_AXE | ~200 | Craft table -> Place Table -> Craft axe |

---

## ğŸ§  Observation Space

| Component | Shape | Description |
|-----------|-------|-------------|
| pov | (4, 84, 84) | Stacked grayscale frames |
| time | (1,) | Normalized time remaining [0, 1] |
| yaw | (1,) | Horizontal rotation [-1, 1] |
| pitch | (1,) | Vertical rotation [-1, 1] |
| place_table_safe | (1,) | Heuristic flag (1.0 if safe to place, else 0.0)|
| inv_logs | (1,) | Inventory count: Logs |
| inv_planks | (1,) | Inventory count: Planks |
| inv_sticks | (1,) | Inventory count: Sticks |
| inv_table | (1,) | Inventory count: Crafting Tables |
| inv_axe | (1,) | Inventory count: Wooden Axes |

---

## ğŸ’° Reward Function

```
reward_per_frame = (logs Ã— wood_value) + step_penalty
```

- **wood_value** points per log (default: 1.0)
- **step_penalty** per MineRL frame (default: -0.001, so -0.004 per decision)
- **axe_reward** axe reward for the first time
- **plank_reward** plank reward for the first time
- **stick_reward** stick reward for the first time
- **waste_penalty** if making stick after the first time punish it

**Example**: Mine 1 log over 4 frames = `(1 Ã— 1.0) + (-0.001 Ã— 4) = +0.996`

---

## ğŸ”„ Wrapper Stack

```
MineRL Base Environment
    â†“
StackAndProcessWrapper (84x84 grayscale, 4 frames)
    â†“
HoldAttackWrapper (attack duration)
    â†“
RewardWrapper (add step penalty)
    â†“
ObservationWrapper (time, yaw, pitch, etc)
    â†“
ConfigurableActionWrapper (22 discrete actions)
    â†“
Agent
```

---

## ğŸ“Š Algorithms

### DQN (Default)
- Double DQN (reduces overestimation)
- Dueling architecture (value + advantage streams)
- Experience replay (100K buffer)
- Epsilon-greedy exploration (1.0 â†’ 0.1)
- Soft target updates (Ï„ = 0.005)

### PPO (Alternative)
- Clipped surrogate objective (Îµ = 0.2)
- GAE advantage estimation (Î» = 0.95)
- Entropy bonus (0.01)
- Rollout buffer (2048 steps)

---
**Test coverage**:
- 47 tests across networks, agent, and wrappers
- Network dimensions, gradient flow, soft updates
- Replay buffer, epsilon schedule, training steps
- Observation wrapper, action wrapper, reward shaping

---

## ğŸ“ˆ Training Flow

1. **Initialize**: Load config, create environment, create agent
2. **Collect**: Agent selects action â†’ env.step() â†’ store experience
3. **Train**: Sample batch â†’ compute loss â†’ update network
4. **Log**: TensorBoard metrics (loss, Q-values, rewards)
5. **Save**: Periodic checkpoints

---

## ğŸ” Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Frame size | 84Ã—84 | Atari standard, good balance |
| Frame stack | 4 | Motion/temporal information |
| Step penalty | Once per decision | Not 4Ã— per frame |
| Macros | Always execute | Learn through experience |
| Inventory | Not observed | Learn from visual hotbar |

---

## ğŸ› ï¸ Technology Stack

- **MineRL 1.0.2** - Minecraft environment
- **PyTorch** - Deep learning
- **OpenCV** - Image processing
- **TensorBoard** - Monitoring
- **pytest** - Testing
